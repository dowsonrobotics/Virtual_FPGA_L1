= Virtual FPGA overall Architecture
Rishiyur S. Nikhil, (c) 2025-2026
:revnumber: v1.0
:revdate: 2025-12-30
:sectnums:
:toc:
:toclevels: 5
:toc: left
:toc-title: Contents
:description: Infrastructure to provide a "virtual FPGA" layer across multiple specific FPGAs
:keywords: FPGA, Custom Logic
:imagesdir: ../Figs
:data-uri:

// ================================================================
// SECTION
== Introduction

We wish to run M *hardware + software* applications ("`apps`") on N
platforms.  By "`platform`" we mean some specific combination of
Host-side (CPU architecture, OS) and HW-side (FPGA board or its
simulation model).

We wish to avoid MxN effort (effort = coding, building, testing), in
favor of M+N effort.  To achieve this, we define a standard "`Virtual
FPGA interface`" for each platform, as shown in the figure below.  The
standard interface is shared across all platforms.  Then, each app
(software and hardware) is designed just once, interacting with this
standard "`Virtual FPGA interface`".  For each platform, we design and
implement the Virtual FPGA once (Layers 1 and 2), offering a standard
interface.

image::RSN_2025-12-17.000.00_virtual_FPGA_arch.png[align="center", width=900]

NOTE: This document is not a User Guide for Virtual FPGA; it is just a
      document on the design philosophy and principles.

// ================================================================
// SECTION
== Details

// ----------------------------------------------------------------
// SUBSECTION
=== Independent subsystems

In the above diagram, the following capabilities are completely
independent of each other, and should be developed and maintained
independently and incrementally:

* Clocks (HW-side) +
All applications need at least one clock

* Multi-queues (Host-HW communication, Host-side and HW-side) +
All applications need multi-queues.

* DDRs (HW-side) +
Many applications need at least one DDR.

* Other board resources (HW-side) +
Some applications may need additional board resources.

// ----------------------------------------------------------------
// SUBSECTION GENERATION
=== Virtual FPGA is _generated_, per application

All communication between App_Host and App_HW are done through a
collection of virtual queues (FIFOs).  For some apps (simple HW
accelerators), that is all they need (no need for DDR or other board
resources).  The number of queues in each direction, and their types,
and their depths, will vary from application to application.

The number of required FPGA-board DDR resources (including possibly
zero) will vary from application to application.  The number and sizes
of available DDR resources will vary from board to board.  For
example, Amazon AWS F1 instances offer 4 DDRs of 16GB each; Xilinx
VCU118 boards offer 2 DDRs of 2GB each.

The number of other required FPGA-board resources (Flash/ROM, UART,
GPIO, Ethernet, USB, ...) will vary from application to application.
The number and type of available resources will vary from board to
board.  Cloud-based FPGAs will likely offer no such resources (beyond
DDR), because there is no physical access to
Flash/UART/GPIO/USB/Ethernet ports.

AXI interfaces to DDR and other board resources will vary from
resource to resource (e.g., AXI id, address, data, and user bus
widths).a

Thus, the Virtual FPGA code is _generated_, per application, from a
JSON file that specifies the details of the resources needed by the
application.

// ----------------------------------------------------------------
// SUBSECTION CLOCKS
=== Clocks for App_HW

Different App_HWs will have different maximum clock speeds at which
they can be synthesized, depending on their circuit complexity and
structure.  Most App_HWs use just one clock, but some may internally
need multiple clocks of different speeds.  The purpose of providing
multiple clocks is to accommodate these variations.

Usually the whole FPGA itself receives a master fast clock from the
board, and a clock-divider in the Board Adapter creates the slower
clocks for App_HW.  FPGA vendors usually provide clock-divider IPs.

The number of clocks, and their clock speeds, may be different across
FPGA boards. This table shows an example of clock speeds that may be
provided:

----
    default clk (CLK)    250   MHz
    clk1                 125   MHz    (CLK / 2)
    clk2                  83.3 MHz    (CLK / 3)
    clk3                  50   MHz    (CLK / 5)
    clk4                  25   MHz    (CLK / 10)
    clk5                  10   MHz    (CLK / 25)
----

*Development tactic:* Initially, provide just the one default clock
Then, refine to support additional clocks.

// ----------------------------------------------------------------
// SUBSECTION MULTI-QUEUE
=== Communication between App_Host and App_HW (Multi-queue interfaces)

Host-HW communication is via the abstraction of a bundle of queues
between Host and FPGA (the number of queues in each direction need not
be the same).  Each queue:

* is _unidirectional_: enqueue on Host and
     pop (dequeue) on FPGA, or vice versa.
* transports discrete, fixed-sized _items_

Each queue is completely flow-controlled (inside the Virtual FPGA
implementation), so there is no danger of overflow or underflow.
Internally, this is implemented using a credit-based flow-control
scheme, so each queue is capable of streaming items from Host-to-HW or
HW-to-Host, avoiding round-trip delays for flow-control.  To wait for
queue availability, both blocking and non-blocking `enqueue()` and
`pop()` API calls are provided.

On the host-side, the multi-queues are thread-safe, and `enqueue()`
and `pop()` API calls are atomic, so App_Host can have multiple
threads (pthreads) interacting with the queues.

_Reactive_ programming is possible by allocating a pthread to perform
a blocking `pop()` call on an incoming queue (from APP_HW).  The
thread wakes when the queue becomes non-empty, and can handle the
incoming item.

On the host-side, components of Virtual FPGA may run on bare metal, in
the kernel, or at user level, depending on the platform.

As shown in the diagram in the Introduction, the Host-FPGA
communication is implemented in two layers.

// ----------------------------------------------------------------
// SUBSUBSECTION LAYER2
==== Layer 2 (Multi-queues) of Host-FPGA communication

Layer 2 is application-specific (number queues and their details) but
largely platform-independent.

Each application specifies how many queues it needs in each direction.
For each queue, it specifies certain fixed parameters:
* the fixed size (bytes) of items in each queue
* the degree of buffering in the queue (queue depth), including how
    this is split between host-side and HW-side.
These parameters may vary from one queue to the next.

Layer 2 depends on a simple, single send/receive interface to Layer 1.
The arguments to send/receive are just an array of bytes and a
byte-count to be sent from/received into the array.  The array size is
fixed to the largest item size amongst the queues in the same
direction.

NOTE: PCIe interfaces support interrupts to the host from the FPGA.
      This can be used internally in Layers 1 and 2 to place an
      incoming item into an L2 receive-queue. This, in turn, will wake
      any app-level pthread waiting on the queue to become non-empty.

*Development tactic:* Implementing PCIe IRQs can be tricky. Intially,
the Virtual FPGA's internal pthread can simply poll for available
messages.

// ----------------------------------------------------------------
// SUBSUBSECTION
==== Layer 1 of Host-FPGA communication

Layer 1 is quite platform specific.

An FPGA board will typically communicate with a host computer over a
PCIe bus, or USB, or Ethernet connection.  For each platform, we have
a version for the actual FPGA board, and a version where the HW runs
in simulation (communicating with over TCP/IP).

[NOTE]
====

PCIe is preferred, for highest bandwidth an lowest latency, but some
FPGA boards will not have PCIe; most will have a USB or Ethernet port.

When using USB/Ethernet, we only need it for the lowest level raw data
transfer.  Specifically, on Ethernet we just need Ethernet packets; we
do not need IP/TCP/UDB/networking.

====

As described in the previous section, Layer 1 offers a simple
send/receive interface to Layer 2.  The arguments to send/receive are
just an array of bytes and a byte-count to be sent from/received into
the array.  The array size is fixed to the largest item size amongst
the queues in the same direction.

*Development tactic:* Initially, restrict to x86 hosts running
Debian/Ubuntu (affects Host-side driver development).  Later, expand
to cover other operating systems (MacOS, Windows, embedded/real-time
OSes, ...)  and other host architectures (ARM, Apple Silicon, RISC-V,
...).

// ----------------------------------------------------------------
// SUBSECTION DDR
=== App_HW access to DDR

The FPGA's DDR memory should be connected to App_HW's AXI interfaces.

Each DDR port on the board should be connected to its own AXI port,
i.e., avoid combining multiple DDRs inside Virtual FPGA to connect to
a single AXI port.  Premature combining can add unnecessary overhead.
App_HW can implement its own combining if it needs to.

The full data width of each DDR port on the board should be preserved
into its AXI port, i.e., avoid expanding/narrowing the data width.
Premature data-width changes can add unnecessary overhead.  App_HW can
implement its own data-width changes if it needs to.

*Development tactic:* Support at least one DDR.  Later, support more
DDRs, if available.

// ----------------------------------------------------------------
// SUBSECTION OTHER RESOURCES
=== App_HW access to other board resources (if available, and if needed)

Same guidelines as for DDR.

// ================================================================
// SECTION Verilog structure
== Board Adapter's Verilog module structure

The figure in the Introduction depicts abstraction layers.  The
following figure shows a more concrete board-and-Verilog-module view.

image::RSN_2025-12-19.000.00_virtual_FPGA_modules.png[align="center", width=800]

In the FPGA, the Board Adapter is the top-level module. It connects
directly to the pins of the FPGA and is shown in the figure as `module
board_adapter`.  It instantiates several Verilog modules and IPs and
connects them up suitably:

* It instantiates the Virtual FPGA infrastructure, connecting it
  ultimately to FPGA pins.  Its BSV interface is `interface
  VF_HW_L2_IFC`.  Note, this interface is generated per-application
  (specifying clocks, multi-queues, AXIs to DDRs and resources).

* It instantiates App_HW (`module mkApp_HW`), passing it the
  `VF_HW_L2_IFC` interface as an argument so that it can interact with
  its methods.

`module board_adapter` and some parts of Virtual FPGA are likely
created using FPGA-vendor specific tools (such as Xilinx Vivado)
because they heavily uses vendor-supplied IPs.

`module mkApp_HW` is the HW-side of the application.  It can be
created in any HDL or combination of HDLs (Verilog, SystemVerilog,
Bluespec BSV/BH, other HDL, ...) provided the result is a Verilog
module with the appropriate interfaces.  It will usually be created in
some other repository, not in this Virtual FPGA repository.

To create the FPGA bitfile, we will provide a build flow that includes
`module board_adapter` and the app's `module mkApp_HW`.

We will provide one or more examples of `module mkApp_HW` for testing
this Virtual FPGA infrastructure (see Testing section below), which
can be used as a template for all other apps.

// ----------------

=== BSV interface `VF_HW_L2_IFC` for Virtual FPGA

BSV interface `VF_HW_L2_IFC` is generated on a per-application basis
based on a JSON file that specifies the resources needed by the
application, and looks like this:

----
interface VF_HW_L2_IFC;
   // ----------------
   method Action start ();

   // ----------------
   // ... Clocks provided to App_HW

   // ----------------
   // Declarations of multi-queue interfaces provided to App_HW

   // ----
   // H2F queue ifc decls
   interface FIFOF_O #(Bit #(H2F_q0_width_b)) fo_h2f_q0;

   // ----
   // F2H queue ifc decls
   interface FIFOF_I #(Bit #(F2H_q0_width_b)) fi_f2h_q0;

   // ----------------
   // ... AXI4 ifcs to DDR(s) 

   // ----------------
   // ... AXI4 ifcs to other resources:
   //     e.g., GPIO, UART, Flash/ROM, Ethernet, USB, ...

   // ----------------
   method Action finish ();
endinterface
----

// ----------------

=== App_HW interface `App_HW_IFC` and module `mkApp_HW`

The user's application HW-side should be a module called `mkApp_HW`
with the interface `App_HW_IFC` shown below:

----
interface App_HW_IFC;
   method ActionValue #(Bool) start ();
   method Bool   done;
   method Action finish ();
endinterface
----

The `start` method is invoked by the board adapter when it is ready to
activate the user application (after reset and other initializations).

The `done` method is an optional convenience; some applications may
have a concept of being "`done`", and this is signaled to the board
adapter for beginning a shutdown process.  It can be hardwired to
return False.

The `finish` method is an optional convenience; if provided, the board
adapter can use it to shut down the application.  It can be be
hardwired to be a no-op.

The user's `mkApp_HW` module should take `VF_HW_L2_IFC` as a module
argument, so that its internals can operate the Virtual FPGA interface
methods.

----
module mkApp_HW  #(VF_HW_L2_IFC l2)  (App_HW_IFC);

   ... uses clocks from App_HW_IFC, if needed ...

   ... enqueues into fi_f2h_q0 to send an item to the host on App_HW-to-App_Host queue 0

   ... pops from fo_h2f_q0 to receive an item from the host on App_Host-to-App_HW queue 0

   ... reads and writes to DDR AXI interface(s)

   ... reads and writes to other resource DDR interfaces
endmodule
----

// ----------------------------------------------------------------
// SUBSECTION
=== Simulation support

For every FPGA board, we should provide a corresponding simulation setup.

In the figure in the Introduction, the Board Adapter layer is
implemented by a Board Adapter _software model_.  This consists of
Verilog that invokes C code to communicate to/from the host and to
model DDRs.  The host communication could be over TCP, shared memory,
pipes, etc.

Correspondingly, the Host-side Driver is corresponding C code (which
can run entirely at user level) and communicates with the
hardware-side simulation process over TCP/shared memory/pipes/...

Ideally, the application code (App_host and App_HW)_FPGA should
require _zero_ changes in moving between simulation and FPGA.
Depending on simulation vs. actual FPGA,

* App_host is simply linked to alternative libraries.

* App_HW is instantiated in alternative Verilog top-levels and
  either taken through a bitfile-build flow or a Verilog simulation
  flow.

Goal: If App_Host and App_HW work in simulation then, assuming
successful FPGA synthesis of App_HW, it should work _immediately_ on
the FPGA, with no changes, no surprises.

// ================================================================
// SECTION
== Build Flows

We would like each application to be portable across multiple
host-sides (architecture, OS) and across multiple FPGA boards.
Further for each such setup, we would like a version that runs
entirely in simulation.

The components are illustrated in the following figure.

image::RSN_2025-12-19.001.00_virtual_FPGA_flows.png[align="center", width=800]

For each application, +
&nbsp;&nbsp;  for each arch/OS host-side, +
&nbsp;&nbsp;&nbsp;&nbsp;    for each FPGA board, +
there will be four builds:

* Host-side executable for simulation
* Host-side executable for FPGA
* FPGA-side executable for simulation
* FPGA-side bitfile for FPGA

The first two will just be Makefiles doing standard compiles and links.

The third will invoke a Verilog compiler (e.g., Verilator) and do compiles and links.

The fourth will involve the FPGA vendor's bitfile build tools running
on Virtual FPGA L1 Verilog (top-level) and the App_HW's Verilog
(instantiated somewhere inside the top-level).

Goals:

* For a given platform (arch/OS + FPGA), between FPGA and simulation
    builds, there should be _zero_ source-code changes in application
    host-side code and App_HW code.

* For a given application, across platforms, there should be _minimal_
    (ideally zero) source-code changes in application host-side code
    and App_HW code.  Some configuration changes may be necessary because
    different platforms may differ in features (e.g., number or
    address ranges of DDRs).

We expect the build directories to be located with the application,
not with the Virtual FPGA directories, since the builds have to be
adapted to user-specific host-side and App_HW code.  The Makefiles
etc. in the Virtual FPGA Test Application (see separate section) can
serve as example templates to be adapted for each application.

// ----------------------------------------------------------------
// SUBSECTION

=== Virtual FPGA specification file

Each application will have its own requirements for:

* Number of clocks (HW-side)

* Number of queues in each direction between Host and HW, and the
  characteristics of each queue:

  ** Byte-width of each item in the queue (size of data transfer in
     each "`enqueue`" or "`dequeue`" transaction)
  ** Capacity of queues on send-side and receive side

* Number and characteristics of AXI memory interfaces, if any.
  Characteristics include AXI address and data bus width.

* Number and characteristics of AXI interfaces to other resources, if
  any (UART, GPIO, ...).  Characteristics include AXI address and data
  bus width.

These are specified as part of the application, declaratively, in a
JSON file (e.g., `App_VF_Spec.json`) at the top-level of the
application repo.

// ----------------------------------------------------------------
// SUBSECTION

=== HW-side build flow, for simulation

The build flow for creating a simulation executable for the HW side is
shown in the following figure:

image::RSN_2026-02-01.000.00_Virtual_FPGA_HW_Sim_Build_flow.png[align="center", width=700]

There is a first `make` step (shown with red arrows) to generate a
certain file from the JSON specification file (shown as
`App_VF_Spec.json` in the diagram).  This is a one-time `make` for a
given JSON spec file.

Then, there is a second `make` step to produce the host-side
executable from the application source files (shown as
`VF_TestApp/Srcs_HW/App_Host.c` in the diagram), the source files
generated in the first step, and other fixed source files.

The command:

----
$ make v_compile v_link
----

creates a Verilator simulation executable, while the command:

----
$ make b_compile b_link
----

creates a Bluesim simulation executable.

These executables run from the command line with no command-line
arguments.  They will immediately listen on a TCP socket for a
connection from the Host-side.

// ----------------------------------------------------------------
// SUBSECTION

=== HW-side build flow, for FPGA

_Details to be written ... but:_

* _the second `make` step shown above for simulation will stop at
    producing Verilog_, and

* that will be followed by integration into boilerplate Verilog and
  taking it through the FPGA flow (synthesis, place & route, bitfile
  generation).

// ----------------------------------------------------------------
// SUBSECTION

=== Host-side build flow, for simulation

The build flow for creating a host-side executable communicating with
a HW-side simulation is shown in the following figure:

image::RSN_2026-02-01.001.00_Virtual_FPGA_Host_Sim_Build_flow.png[align="center", width=700]

There is a first `make` step (shown with red arrows) to generate
certain files from the JSON specification file (shown as
`App_VF_Spec.json` in the diagram).  This is a one-time `make` for a
given JSON spec file.

Then, there is a second `make` step to produce either a Verilator or a
Bluesim simulation executable, which uses the application source files
(shown as `VF_TestApp/Srcs_HW/App_HW.bsv` in the diagram), the source
files generated in the first step, and other fixed source files.

The command:

----
$ make all
----

creates the host-side executable.

This executable runs from the command line and will connect to a
HW-side simulation executable that is listening on a TCP socket.

// ----------------------------------------------------------------
// SUBSECTION

=== Host-side build flow, for FPGA

_Details to be written ..._

// ================================================================
// SECTION
== Virtual FPGA Test Application

We provide one or more small Test Applications "`TestApp`"for the
Virtual FPGA infrastructure.

* FPGA-side: a simple App_HW, written in BSV, that simply connects
  the host-communication AXI4_S interface to the DDR AXI4_M interfaces
  via an AXI4 switch, straightforwardly mapping AXI4_S addresses to
  AXI4_M addresses.

* Host-side: a simple C program with `vf_read()/vf_write()`
  invocations, which eventually become AXI reads and writes on the
  DDRs on the FPGA-side.  The C program checks for correct memory
  operation.  The code should perform millions of reads/writes, to
  random legal addresses, with random legal sizes, and to specific
  directed corner cases (address boundaries of the DDRs, page
  boundaries, AXI4 lane-alignment boundaries, etc.).

We will provide the four build flows for each supported platform
(arch/OS + FPGA): build host-side for FPGA and for simulation, build
App_HW for FPGA and for simulation.

// ================================================================
// SECTION
== Board Adapter Data Sheet

The Board Adapter data sheet for a particular platform should document
which resources are available for that platform.

// ================================================================
