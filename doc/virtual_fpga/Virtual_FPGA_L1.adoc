= Virtual FPGA overall Architecture
Rishiyur S. Nikhil, (c) 2025-2026
:revnumber: v1.0
:revdate: 2025-12-30
:sectnums:
:toc:
:toclevels: 5
:toc: left
:toc-title: Contents
:description: Infrastructure to provide a "virtual FPGA" layer across multiple specific FPGAs
:keywords: FPGA, Custom Logic
:imagesdir: ../Figs
:data-uri:

// ================================================================
// SECTION
== Introduction

We wish to run M *hardware + software* applications ("`apps`") on N
platforms (OS, FPGA board, host-to-FPGA communication medium) with M+N
effort instead of MxN effort (effort = coding, building, testing).  To
achieve this, we define a standard "`Virtual FPGA interface`" for each
platform, as shown in the figure below.  The standard interface is
shared across all platforms.  Then, each app (software and hardware)
is designed just once, interacting with this standard "`Virtual FPGA
interface`".  For each platform, we design and implement its "`Board
Adapter`" and host-side drivers once, offering this standard
interface.

image::RSN_2025-12-17.000.00_virtual_FPGA_arch.png[align="center", width=900]

The host-side drivers may run at OS level (e.g., to access PCIe) or at
user-level (e.g., for TCP for hardware simulation).

// ================================================================
// SECTION
== Details

In the above diagram, the following on FPGA-side capabilities are
completely independent of each other, and can be developed
independently and incrementally:

1. App_HW to App_Host communication (Host-FPGA communication)
2. App_HW to DDR access
3. App_HW to other board resources

Note that for (2) and (3), the various AXI ports may have different
parameters (bus widths, burst support, ...)

Note that for FPGAs in the cloud (e.g., Amazon AWS F1), capabilities
(3) do not exist; all services are provided by proxy from the host via
communication over (1), for example using Virtio.

In the sections below, paragraphs introduced with "`*development
tactic*`" suggest a minimal initial implementation, to be refined
later into a more full-function and optimized implementation.  This
will enable a quick bring-up of a new host/board combo with minimal
but usable functionality, to be fleshed out later over time.

A useful initial development goal: Host-FPGA communication, and access
to one DDR.  This is reflected in the order of sections below.

// ----------------------------------------------------------------
// SUBSECTION HOST COMMUNICATION
=== Data communication to/from with Host (Host-side and FPGA-side)

As shown in the diagram, the Host-FPGA communications are implemented
in two layers:

// ----------------------------------------------------------------
// SUBSUBSECTION
==== Layer 2 of Host-FPGA communication

Layer 2 provides the abstraction of a bundle of queues between Host
and FPGA, and vice-versa (the number of queues in each direction need
not be the same).  Each queue:

* is _unidirectional_: enqueue on Host and
     dequeue on FPGA, or vice versa.
* transports discrete, fixed-sized _items_

Each application specifies how many queues it needs in each direction.
For each queue, it specifies certain fixed parameters:
* the size of items in each queue
* the degree of buffering in the queue (queue depth)
These parameters may vary from one queue to the next.

Layer 2 is expected to be _generated_ (e.g., from a Python script)
according to the above App-specific parameters.

Layer 2 is expected to be largely _platform-independent_, implementing
a _credit-based flow-control_ scheme for each queue.

Layer 2 depends on a simple send/receive interface to Layer 1.  The
arguments to send/receive are just an array of bytes and a byte-count
to be sent from/received into the array.  The array can be a fixed
size, equal to the largest item amongst the queues in the same
direction.

// ----------------------------------------------------------------
// SUBSUBSECTION
==== Layer 1 of Host-FPGA communication

Layer 1 is highly platform dependent.

An FPGA board will typically communicate with a host computer over a
PCIe bus, or USB or Ethernet connections.

[NOTE]
====

PCIe is preferred, for highest bandwidth an lowest latency, but some
FPGA boards will not have PCIe; most will have a USB or Ethernet port.

When using USB/Ethernet, we only need it for the lowest level raw data
transfer.  Specifically, on Ethernet we would just communicate
Ethernet packets, with no IP/TCP/UDB/networking layers above that.

====

As described in the previous section, Layer 1 offers a simple
send/receive interface to Layer 2.  The arguments to send/receive are
just an array of bytes and a byte-count to be sent from/received into
the array.  The array can be a fixed size, equal to the largest item
amongst the queues in the same direction.

*Development tactic:* Initially, restrict to x86 hosts running
Debian/Ubuntu (affects Host-side driver development).  Later, expand
to cover other operating systems (MacOS, Windows, embedded/real-time
OSes, ...)  and other host architectures (ARM, Apple Silicon, RISC-V,
...).

// ----------------------------------------------------------------
// SUBSUBSECTION INTERRUPTS
==== Interrupting the host

For at least one, and possibly all the FPGA-to-Host direction queues,
it should be possible to run an interrupt handler in App_Host space
when the queue goes from empty to non-empty, so that App_Host does not
have to poll the queues for arrival of an item.

*Development tactic:* Do not implement any IRQ; add it later.  In the
meanwhile, Host_App can poll the queues for item arrival.

// ----------------------------------------------------------------
// SUBSECTION DDR
=== DDR Memory

The FPGA's DDR memory should be connected to App_HW's AXI4_M (AXI4
Manager) interfaces.

Each DDR port on the board should be preserved and connected to its
own App_HW AXI4_M port, i.e., do not combine multiple DDRs to
connect to a single AXI4_M port.  Premature combining can add
unnecessary overhead.  App_HW can implement its own combining if it
needs to.

The full data width of each DDR port on the board should be preserved
into its App_HW AXI4_M port, i.e., do not expand/narrow the data
width.  Premature width-changes can add unnecessary overhead.
App_HW can implement its own width-changes if it needs to.

It is expected that FPGA boards will vary on:

* Number of DDR ports
* Size of address range on each DDR port
* Width of data bus on each DDR port
* Speed of each DDR port

The above details should be provided on the datasheet for each Board
Adapter.

IMPORTANT: Please review the IMPORTANT note at the top of the Details
    section to remind that DDR AXI4_Ms are completely independent from
    the host AXI4_S.

*Development tactic:* Support at least one DDR.  Later, support more
DDRs, if available.

// ----------------------------------------------------------------
// SUBSECTION CLOCKS
=== Clocks for App_HW

Different App_HWs will have different maximum clock speeds at which
they can be synthesized, depending on their circuit complexity and
structure.  Further, some App_HWs may internally need clocks of
different speeds.  Most App_HWs use just one clock.

The purpose of providing multiple clocks is to accommodate these
variations.

Usually the whole FPGA itself receives a master fast clock from the
board, and a clock-divider in the Board Adapter creates the slower
clocks for App_HW.  FPGA vendors usually provide clock-divider IPs.

The number of clocks, and their clock speeds, may be different across
FPGA boards. This table shows an example of clock speeds that may be
provided:

----
    default clk (CLK)    250   MHz
    clk1                 125   MHz    (CLK / 2)
    clk2                  83.3 MHz    (CLK / 3)
    clk3                  50   MHz    (CLK / 5)
    clk4                  25   MHz    (CLK / 10)
    clk5                  10   MHz    (CLK / 25)
----

*Development tactic:* Initially, provide just the one default clock
Then, refine to support additional clocks.

// ----------------------------------------------------------------
// SUBSECTION
=== Optional FPGA resources

If the FPGA board contains other resources, they should be accessible
through AXI4_M interfaces on App_HW.  Examples:

* Flash/ROM
* UART
* GPIO

* Other Ethernet/USB connection +
Note, this is for actual, normal Ethernet/USB communication (if
required) and should not be confused with the main Virtual FPGA
host-communication.

* Sensors, actuators, ...

*Development tactic:* Initially, do not provide any optional FPGA
resources. Add them later, if needed.  *Note:* for FPGAs in the cloud,
such as Amazon F1/F2 instances, there are no such resources;
everything has to be done through the host communication interface.

// ================================================================
// SECTION Verilog structure
== Board Adapter's Verilog module structure

The figure in the Introduction depicts abstraction layers.  The
following figure shows a more concrete board-and-Verilog-module view.

image::RSN_2025-12-19.000.00_virtual_FPGA_modules.png[align="center", width=800]

In the FPGA, the Board Adapter is the top-level module. It connects
directly to the pins of the FPGA and is shown in the figure as `module
board_adapter`.  It instantiates several Verilog modules and IPs and
connects them up suitably:

* The App_HW, shown as `module mkApp_HW`

* IPs needed by the App_HW, for clocks, PCIe/USB/Ethernet host
  connectivity, DDRs, optional IPs for Flash/ROM, UART, GPIO,
  external USB/Ethernet connections, ... +
  These IPs are usually provided in the FPGA vendor's libraries.

`module board_adapter` is likely created using FPGA-vendor specific
tools (such as Xilinx Vivado) because it heavily uses vendor-supplied
IPs.  It should instantiate `module mkApp_HW` and connect to its
ports.

`module mkApp_HW` is application-specific.  It can be created in any
HDL or combination of HDLs (Verilog, SystemVerilog, Bluespec BSV/BH,
other HDL, ...) provided the result is a Verilog module with the
appropriate AXI4 and other interfaces.  It will usually be created in
some other repository, not in this Virtual FPGA repository.

To create the FPGA bitfile, we will provide a build flow that includes
`module board_adapter` and the app's `module mkApp_HW`.

We will provide a few examples of `module mkApp_HW` for testing this
Virtual FPGA infrastructure (see Testing section below), which can be
used as a template for all other apps.

// ----------------
=== Interface for `mkApp_HW`

WARNING: This section needs rewriting. The TestApps are a better way
         to show what the interface looks like, which now has FIFOs
         to/from host instead of AXI4.

In the directory `Interface_Spec` we provide Verilog for an example
"`empty`" `mkApp_HW` that can be used as a proxy for the real
App_HW during Board Adapter development.

The Verilog file is generated from the following BSV code, in file
`App_HW.bsv`.

----
interface App_HW_IFC;
   // AXI4 from host
   interface AXI4_RTL_S_IFC #(16, 64, 512, 0) host_AXI4_S;

   // Interrupts to host
   interface FIFOF_O #(Bit #(64)) tohost_interrupts;

    // DDR interfaces
   interface AXI4_RTL_M_IFC #(16, 64, 512, 0) ddr_A_M;
   interface AXI4_RTL_M_IFC #(16, 64, 512, 0) ddr_B_M;
endinterface

module mkApp_HW
   #(Clock clk1, Clock clk2, Clock clk3, Clock clk4, Clock clk5)
   (App_HW_IFC);

   // ... empty, for this proxy generator

endmodule
----

In the `Interface_Spec/` directory, running `make v_compile` will
create `verilog/mkApp_HW.v` containing a Verilog module
`mkApp_HW`.  Its body is empty (contains no App_HW logic) but it
has the required input and output ports according the the figure in
the Introduction, so it serves as a specification of the interface
between the Board Adapter and the App_HW.

[NOTE]
====
This is just an example, incorporating specific choices:

* It assumes the Board Adapter provides five clocks
* It has two DDR interfaces
* The host and DDR interfaces have particular widths for the AXI4 buses

This example should be treated as a template, and should be adjusted
to match a particular Board Adapter, which may be differ in these
choices.
====

// ----------------------------------------------------------------
// SUBSECTION
=== Simulation support

For every FPGA board, we should provide a corresponding simulation setup.

In the figure shown in Section 1, the Board Adapter layer is
implemented by a Board Adapter _software model_.  This consists of
Verilog that invokes C code to communicates to/from the host and to
model DDRs.  The host communication could be over TCP, shared memory,
pipes, etc.

Correspondingly, the Host-side Driver is corresponding C code (which
can run entirely at user level) and communicaes with the hardware-side
simulation process over TCP/shared memory/pipes/...

Ideally, the application code (App_host and App_HW)_FPGA should
require _zero_ changes, whether in simulation or on FPGA.  
Depending on simulation vs. actual FPGA,

* App_host is simply linked to alternative libraries.

* App_HW is instantiated in alternative Verilog top-levels and
  either taken through a bitfile-build flow or a Verilog simulation
  flow.

Goal: If the host-side application code and the App_HW work in simulation
then, assuming successful FPGA synthesis of the App_HW, it should work
_immediately_ on the FPGA, with no changes, no surprises.

// ================================================================
// SECTION
== Build Flows

We would like each application to be portable across multiple
host-sides (architecture, OS) and across multiple FPGA boards.
Further for each such setup, we would like a version that runs
entirely in simulation.

The components are illustrated in the following figure.

image::RSN_2025-12-19.001.00_virtual_FPGA_flows.png[align="center", width=800]

For each application, +
&nbsp;&nbsp;  for each arch/OS host-side, +
&nbsp;&nbsp;&nbsp;&nbsp;    for each FPGA board, +
there will be four builds:

* Host-side executable for simulation
* Host-side executable for FPGA
* FPGA-side executable for simulation
* FPGA-side bitfile for FPGA

The first two will just be Makefiles doing standard compiles and links.

The third will invoke a Verilog compiler (e.g., Verilator) and do compiles and links.

The fourth will involve the FPGA vendor's bitfile build tools running
on Virtual FPGA L1 Verilog (top-level) and the App_HW's Verilog
(instantiated somewhere inside the top-level).

Goals:

* For a given platform (arch/OS + FPGA), between FPGA and simulation
    builds, there should be _zero_ source-code changes in application
    host-side code and App_HW code.

* For a given application, across platforms, there should be _minimal_
    (ideally zero) source-code changes in application host-side code
    and App_HW code.  Some configuration changes may be necessary because
    different platforms may differ in features (e.g., number or
    address ranges of DDRs).

We expect the build directories to be located with the application,
not with the Virtual FPGA directories, since the builds have to be
adapted to user-specific host-side and App_HW code.  The Makefiles
etc. in the Virtual FPGA Test Application (see separate section) can
serve as example templates to be adapted for each application.

// ================================================================
// SECTION
== Virtual FPGA Test Application

We will provide a small suite of Test Applications "`TestApp`"for the
Virtual FPGA infrastructure.

* FPGA-side: a simple App_HW, written in BSV, that simply connects
  the host-communication AXI4_S interface to the DDR AXI4_M interfaces
  via an AXI4 switch, straightforwardly mapping AXI4_S addresses to
  AXI4_M addresses.

* Host-side: a simple C program with `vf_read()/vf_write()`
  invocations, which eventually become AXI reads and writes on the
  DDRs on the FPGA-side.  The C program checks for correct memory
  operation.  The code should perform millions of reads/writes, to
  random legal addresses, with random legal sizes, and to specific
  directed corner cases (address boundaries of the DDRs, page
  boundaries, AXI4 lane-alignment boundaries, etc.).

We will provide the four build flows for each supported platform
(arch/OS + FPGA): build host-side for FPGA and for simulation, build
App_HW for FPGA and for simulation.

// ================================================================
// SECTION
== Host-to-FPGA communications

WARNING: This section needs to be rewritten, to discuss the
    Multi-Queue interface (used to be single AXI4).

This section focuses on communications between the host and FPGA,
illustrated in the following figure.

image::RSN_2025-12-28.000.00_virtual_FPGA_host_comms.png[align="center", width=800]

NOTE: We reemphasize the IMPORTANT note at the head of the Details
      section that App_host to App_HW communication is _completely
      independent_ of App_HW to DDR access (and hence DDR access is
      not even shown in the above figure). Please re-read that note,
      if necessary.

`vf_read()` and `vf_write()` calls in App_host are communicated to the
FPGA-side where they are presented as AXI4 transactions on App_HW's
AXI4_S port.

There is no restriction on `buf` size, nor on `addr` alignment.

The figure shows the conversion to AXI4 happening on FPGA-side, but
some of the conversion can happen host-side.  For example, since AXI4
transactions are not allowed to cross 4K page boundaries, splitting
into pages may be performed on host-side, and final AXI4 conversion
performed FPGA-side.

`vf_read()/vf_write()` transactions have to be converted into one or
more AXI4 bursts, with data width on each beat limited to the AXI4_S
data bus width.  Further, these have to be lane-aligned according to
AXI4 rules.

AXI4 transactions may return errors, which are returned to the host.
In read-transactions, which may require bursts or even multiple
bursts, an AXI4 error may be returned well into the tranaction.  To
avoid data size ambiguity, we always return `size` data, appended by
the OK/ERR indicator.  In the case of error, the host should discard
the data.

// ================================================================
// SECTION
== Board Adapter Data Sheet

The Board Adapter data sheet should document which features are implemented,
and with what parameters, e.g.,

* Number of provided clocks, their speeds
* Host-communication AXI4_S bus widths; whether or not bursts are supported
* Whether or not FPGA-to-host-interrupts are supported
* Number of DDR4 AXI4_M interfaces, their bus widths and supported address ranges

The Board Adapter data sheet should document how to build the
FPGA-side bitfile, assuming we have the App_HW Verilog with matching
Board Adapter interface.

The Board Adapter data should specify how to build the host-side
executable (how to link to/invoke the System Support API).

The host-side System Support for each board should support an API-call
to discover the data sheet information.

// ================================================================
